{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:52.131796600Z",
     "start_time": "2025-12-19T15:13:51.917083700Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:56.082450200Z",
     "start_time": "2025-12-19T15:13:52.132795500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retrieve Bike Counting Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "MONTH = '04'\n",
    "YEAR = '2023'\n",
    "START_DATE = f'{YEAR}-01-01'\n",
    "END_DATE = f'{YEAR}-12-31'"
   ],
   "id": "783468df00c025e6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Requête des données",
   "id": "5557c2a9ef97c466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:58.138332100Z",
     "start_time": "2025-12-19T15:13:56.137710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "channels_data = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"sep\", \";\") \\\n",
    "               .csv(\"./CSVs/channels.csv\")\n",
    "\n",
    "channels_ID = [row['channel_id'] for row in channels_data.collect()]"
   ],
   "id": "e607fe0d98606298",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:58.158425800Z",
     "start_time": "2025-12-19T15:13:58.149808300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def request_counting_data(channel_id, start_date, end_date):\n",
    "    url = f'https://data.grandlyon.com/fr/datapusher/ws/timeseries/pvo_patrimoine_voirie.pvocomptagemeasure/all.json?start_datetime__gte={start_date}&start_datetime__lt={end_date}&channel_id__eq={channel_id}&maxfeatures=-1'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('values', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve data for channel {channel_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def request_temp_data(start_date, end_date):\n",
    "    url = f'https://data.grandlyon.com/fr/datapusher/ws/timeseries/biotope.temperature/all.json?horodate__gte={start_date}&horodate__lt={end_date}&maxfeatures=-1'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('values', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve temperature data: {e}\")\n",
    "        return []\n",
    "\n",
    "def request_rain_data(start_date, end_date):\n",
    "    url = f'https://data.grandlyon.com/fr/datapusher/ws/timeseries/eau.pluviometrie_mesure/all.json?maxfeatures=-1&horodate__gte={start_date}&horodate__lte={end_date}'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('values', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve rainfall data: {e}\")\n",
    "        return []"
   ],
   "id": "93c37c3321137143",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Traitement des données",
   "id": "9a25345b1480af6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Traitement des données de comptage",
   "id": "86c426feab5b2bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:58.166462500Z",
     "start_time": "2025-12-19T15:13:58.159425400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aggréger les données de comptage de la même heure toutes stations confondues\n",
    "def traitement_count_data(data):\n",
    "    data_with_hour = data.withColumn(\"start_datetime\", F.date_trunc(\"hour\", F.col(\"start_datetime\")))\n",
    "    count_data_agg = data_with_hour.groupBy(\"start_datetime\") \\\n",
    "        .agg(F.sum(\"count\").alias(\"total_count\")) \\\n",
    "        .orderBy(\"start_datetime\")\n",
    "    count_data_agg.drop(\"end_datetime\")\n",
    "    count_data_agg = count_data_agg.withColumnRenamed(\"start_datetime\", \"horodate_heure\")\n",
    "    return count_data_agg"
   ],
   "id": "a0ae33d43e769f15",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Traitement des données de température",
   "id": "392998cd5dffd4e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:58.175419700Z",
     "start_time": "2025-12-19T15:13:58.167722200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aggréger les données de température par horodate selon l'heure et la station de mesure\n",
    "def traitement_temp_data(data):\n",
    "    data_with_hour = data.withColumn(\"horodate_heure\", F.date_trunc(\"hour\", F.col(\"horodate\")))\n",
    "    temp_data_agg = data_with_hour.groupBy(\"horodate_heure\") \\\n",
    "        .agg(F.avg(\"degre_celsius\").alias(\"avg_temp_per_hour\")) \\\n",
    "        .orderBy(\"horodate_heure\")\n",
    "    temp_data_agg.drop(\"horodate\")\n",
    "    return temp_data_agg"
   ],
   "id": "309b4e4580a06b48",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Traitement des données de précipitations",
   "id": "9747039d87187a2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:13:58.184000300Z",
     "start_time": "2025-12-19T15:13:58.175419700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aggréger les données de précipitations d'une même station de mesure\n",
    "def traitement_rainfall(data):\n",
    "    data_with_hour = data.withColumn(\"horodate_heure\", F.date_trunc(\"hour\", F.col(\"horodate\")))\n",
    "    rain_data_agg_station = data_with_hour.groupBy(\"horodate_heure\", \"identifiant\") \\\n",
    "        .agg(F.sum(\"pluie_mm\").alias(\"avg_pluie_per_station_per_hour\"))\n",
    "    rain_data_agg = rain_data_agg_station.groupBy(\"horodate_heure\") \\\n",
    "        .agg(F.avg(\"avg_pluie_per_station_per_hour\").alias(\"avg_pluie_per_hour\")) \\\n",
    "        .orderBy(\"horodate_heure\")\n",
    "    rain_data_agg.drop(\"horodate\")\n",
    "    return rain_data_agg\n"
   ],
   "id": "a25fd3f8ef3176d7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exécution du traitement",
   "id": "d5a5e4b432ac1691"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:14:57.595391200Z",
     "start_time": "2025-12-19T15:13:58.217233300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_rdd_ids = spark.sparkContext.parallelize(channels_ID)\n",
    "count_records_rdd = count_rdd_ids.flatMap(lambda cid: request_counting_data(cid, START_DATE, END_DATE))\n",
    "count_data_spark = spark.createDataFrame(count_records_rdd)\n",
    "aggregated_count_data = traitement_count_data(count_data_spark)\n",
    "aggregated_count_data.show()"
   ],
   "id": "8c3bcff98d33127d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|     horodate_heure|total_count|\n",
      "+-------------------+-----------+\n",
      "|2023-01-01 00:00:00|       1888|\n",
      "|2023-01-01 01:00:00|       2283|\n",
      "|2023-01-01 02:00:00|       2408|\n",
      "|2023-01-01 03:00:00|       2694|\n",
      "|2023-01-01 04:00:00|       1998|\n",
      "|2023-01-01 05:00:00|       1501|\n",
      "|2023-01-01 06:00:00|       1041|\n",
      "|2023-01-01 07:00:00|        744|\n",
      "|2023-01-01 08:00:00|       1052|\n",
      "|2023-01-01 09:00:00|       1524|\n",
      "|2023-01-01 10:00:00|       2652|\n",
      "|2023-01-01 11:00:00|       4311|\n",
      "|2023-01-01 12:00:00|       5022|\n",
      "|2023-01-01 13:00:00|       4816|\n",
      "|2023-01-01 14:00:00|       6020|\n",
      "|2023-01-01 15:00:00|       7214|\n",
      "|2023-01-01 16:00:00|       6399|\n",
      "|2023-01-01 17:00:00|       5123|\n",
      "|2023-01-01 18:00:00|       4599|\n",
      "|2023-01-01 19:00:00|       4552|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:15:22.233383Z",
     "start_time": "2025-12-19T15:14:57.651056700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_records = request_temp_data(START_DATE, END_DATE)\n",
    "temp_data_spark = spark.createDataFrame(temp_records)\n",
    "aggregated_temp_data = traitement_temp_data(temp_data_spark)\n",
    "aggregated_temp_data = aggregated_temp_data.withColumn(\"avg_temp_per_hour\", F.round(F.col(\"avg_temp_per_hour\"), 2))\n",
    "aggregated_temp_data.show()"
   ],
   "id": "25afbf9c49f31a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|     horodate_heure|avg_temp_per_hour|\n",
      "+-------------------+-----------------+\n",
      "|2023-01-01 00:00:00|            15.14|\n",
      "|2023-01-01 01:00:00|            14.91|\n",
      "|2023-01-01 02:00:00|            14.72|\n",
      "|2023-01-01 03:00:00|            14.88|\n",
      "|2023-01-01 04:00:00|            14.89|\n",
      "|2023-01-01 05:00:00|            14.78|\n",
      "|2023-01-01 06:00:00|            14.59|\n",
      "|2023-01-01 07:00:00|            14.43|\n",
      "|2023-01-01 08:00:00|             14.4|\n",
      "|2023-01-01 09:00:00|            14.83|\n",
      "|2023-01-01 10:00:00|            15.71|\n",
      "|2023-01-01 11:00:00|            16.58|\n",
      "|2023-01-01 12:00:00|            16.87|\n",
      "|2023-01-01 13:00:00|            17.04|\n",
      "|2023-01-01 14:00:00|            16.85|\n",
      "|2023-01-01 15:00:00|            16.34|\n",
      "|2023-01-01 16:00:00|            16.01|\n",
      "|2023-01-01 17:00:00|            15.67|\n",
      "|2023-01-01 18:00:00|            15.63|\n",
      "|2023-01-01 19:00:00|            15.46|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:15:42.603211400Z",
     "start_time": "2025-12-19T15:15:22.290209800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Partie Meteo\n",
    "rain_records = request_rain_data(START_DATE, END_DATE)\n",
    "rain_data_spark = spark.createDataFrame(rain_records)\n",
    "aggregated_rain_data = traitement_rainfall(rain_data_spark)\n",
    "aggregated_rain_data = aggregated_rain_data.withColumn(\"avg_pluie_per_hour\", F.round(F.col(\"avg_pluie_per_hour\"), 2))\n",
    "aggregated_rain_data.show()"
   ],
   "id": "fcbd0ccc99887d55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|     horodate_heure|avg_pluie_per_hour|\n",
      "+-------------------+------------------+\n",
      "|2023-01-01 08:00:00|               0.3|\n",
      "|2023-01-01 09:00:00|               0.1|\n",
      "|2023-01-01 14:00:00|               0.1|\n",
      "|2023-01-01 19:00:00|               0.1|\n",
      "|2023-01-02 04:00:00|               0.1|\n",
      "|2023-01-02 05:00:00|               0.1|\n",
      "|2023-01-02 13:00:00|              0.17|\n",
      "|2023-01-02 14:00:00|               0.5|\n",
      "|2023-01-02 15:00:00|               0.2|\n",
      "|2023-01-02 16:00:00|              0.15|\n",
      "|2023-01-02 17:00:00|              0.88|\n",
      "|2023-01-02 18:00:00|              2.02|\n",
      "|2023-01-02 19:00:00|              1.89|\n",
      "|2023-01-02 20:00:00|              1.09|\n",
      "|2023-01-02 21:00:00|              0.51|\n",
      "|2023-01-02 22:00:00|               0.2|\n",
      "|2023-01-02 23:00:00|              0.87|\n",
      "|2023-01-03 00:00:00|               0.6|\n",
      "|2023-01-03 01:00:00|              0.23|\n",
      "|2023-01-03 02:00:00|              0.27|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:16:31.421038200Z",
     "start_time": "2025-12-19T15:15:42.632374900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_data = aggregated_count_data.join(aggregated_temp_data, \"horodate_heure\", \"left\") \\\n",
    "                                  .join(aggregated_rain_data, \"horodate_heure\", \"left\") \\\n",
    "                                  .withColumnRenamed(\"horodate_heure\", \"datetime\") \\\n",
    "                                  .orderBy(\"datetime\")\n",
    "final_data = final_data.fillna(0)\n",
    "final_data.show(20)"
   ],
   "id": "7781669ef042c4da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------------+------------------+\n",
      "|           datetime|total_count|avg_temp_per_hour|avg_pluie_per_hour|\n",
      "+-------------------+-----------+-----------------+------------------+\n",
      "|2023-01-01 00:00:00|       1888|            15.14|               0.0|\n",
      "|2023-01-01 01:00:00|       2283|            14.91|               0.0|\n",
      "|2023-01-01 02:00:00|       2408|            14.72|               0.0|\n",
      "|2023-01-01 03:00:00|       2694|            14.88|               0.0|\n",
      "|2023-01-01 04:00:00|       1998|            14.89|               0.0|\n",
      "|2023-01-01 05:00:00|       1501|            14.78|               0.0|\n",
      "|2023-01-01 06:00:00|       1041|            14.59|               0.0|\n",
      "|2023-01-01 07:00:00|        744|            14.43|               0.0|\n",
      "|2023-01-01 08:00:00|       1052|             14.4|               0.3|\n",
      "|2023-01-01 09:00:00|       1524|            14.83|               0.1|\n",
      "|2023-01-01 10:00:00|       2652|            15.71|               0.0|\n",
      "|2023-01-01 11:00:00|       4311|            16.58|               0.0|\n",
      "|2023-01-01 12:00:00|       5022|            16.87|               0.0|\n",
      "|2023-01-01 13:00:00|       4816|            17.04|               0.0|\n",
      "|2023-01-01 14:00:00|       6020|            16.85|               0.1|\n",
      "|2023-01-01 15:00:00|       7214|            16.34|               0.0|\n",
      "|2023-01-01 16:00:00|       6399|            16.01|               0.0|\n",
      "|2023-01-01 17:00:00|       5123|            15.67|               0.0|\n",
      "|2023-01-01 18:00:00|       4599|            15.63|               0.0|\n",
      "|2023-01-01 19:00:00|       4552|            15.46|               0.1|\n",
      "+-------------------+-----------+-----------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T15:19:57.044566200Z",
     "start_time": "2025-12-19T15:18:48.973680100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_data.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(f\"./CSVs/spark/counting_data{YEAR}\")"
   ],
   "id": "a5ffbf3bcdcac4e7",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
